import { container } from '@/src/di/container';
import { db } from '@/lib/firebase'; // Keep for system prompt fetching only (temporary)
import { doc, getDoc } from 'firebase/firestore';

// In-memory rate limiter
const requestsMap = new Map<string, number[]>();

export async function POST(req: Request) {
    try {
        // 1. Rate Limiting
        const ip = req.headers.get("x-forwarded-for") ?? "unknown";
        const now = Date.now();
        const windowMs = 60_000;
        const rateLimit = 10; // Relaxed a bit for stability

        const timestamps = (requestsMap.get(ip) ?? []).filter(
            (t) => now - t < windowMs
        );

        if (timestamps.length >= rateLimit) {
            return new Response(
                JSON.stringify({
                    error: "çŸ­æ™‚é–“ã«ã‚¢ã‚¯ã‚»ã‚¹ãŒé›†ä¸­ã—ã¦ã„ã¾ã™ã€‚ã—ã°ã‚‰ãæ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                }),
                {
                    status: 429,
                    headers: { "Content-Type": "application/json" },
                }
            );
        }
        timestamps.push(now);
        requestsMap.set(ip, timestamps);

        // 2. Parse Request
        const { messages, model: requestedModel, mentorMode, stamina } = await req.json() as {
            messages: { role: "user" | "assistant"; content: string }[];
            model?: string;
            mentorMode?: boolean;
            stamina?: number;
        };

        if (!messages || !Array.isArray(messages) || messages.length === 0) {
            return new Response(JSON.stringify({ error: "Invalid messages format" }), {
                status: 400,
                headers: { "Content-Type": "application/json" },
            });
        }

        const lastUserMessage = messages.slice().reverse().find(m => m.role === 'user');
        const userQuery = lastUserMessage ? lastUserMessage.content : "";

        // Logging variables
        const start = Date.now();
        let responseText = "";
        let success = false;
        let errorMessage: string | null = null;
        let ragSources: any[] = [];

        try {
            // 3. RAG Retrieval (Vertex AI Search & Supabase Knowledge)
            const ragService = container.ragService;
            const knowledgeRepo = container.knowledgeRepository;

            // Parallel fetch: Vertex AI Search + Top Knowledge from Supabase
            const [vertexResponse, topKnowledge] = await Promise.all([
                ragService.search(userQuery),
                knowledgeRepo.getTopKnowledge(5)
            ]);

            ragSources = vertexResponse.citations;

            // 4. Construct Context
            // Firestoreã‹ã‚‰ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾— (ã“ã“ã ã‘ç§»è¡Œéæ¸¡æœŸã¨ã—ã¦ç¶­æŒ)
            let systemPromptContent = `ã‚ãªãŸã¯OWLightã®è³¢è€…ã€ŒMr.OWLã€ã§ã™ã€‚è‡ªæ²»ä½“è·å“¡ã®ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ã—ã¦ã€ä¸å¯§ã‹ã¤æ¸©ã‹ã„ã€Œæ©é€ã‚Šï¼ˆPay it Forwardï¼‰ã€ã®ç²¾ç¥ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚`;
            try {
                const docRef = doc(db, 'settings', 'system_prompt');
                const docSnap = await getDoc(docRef);
                if (docSnap.exists() && docSnap.data().content) {
                    systemPromptContent = docSnap.data().content;
                }
            } catch (error) {
                console.error("Failed to fetch system prompt from Firestore, using default.", error);
            }

            // Knowledge Context (from Supabase)
            let knowledgeContext = "";
            if (topKnowledge.length > 0) {
                const list = topKnowledge.map(d => {
                    return `- ${d.content} (é–¢é€£ã‚¿ã‚°:${d.structuredData?.tags?.join(', ') || 'ãªã—'})`;
                }).join('\n');
                knowledgeContext = `\n\n### åºå†…ã®å…±æœ‰ãƒŠãƒ¬ãƒƒã‚¸ (ç¾å ´ã®çŸ¥æµ)\nä»¥ä¸‹ã®æƒ…å ±ã¯ã€ç¾å ´è·å“¡ã«ã‚ˆã£ã¦é«˜ãè©•ä¾¡ã•ã‚ŒãŸé‡è¦ãªçŸ¥æµã§ã™ã€‚é–¢é€£æ€§ãŒé«˜ã‘ã‚Œã°å‚ç…§ã—ã¦ãã ã•ã„ï¼š\n${list}`;
            }

            // Vertex AI Search Context
            let vertexContext = "";
            console.log("[ChatAPI] Vertex Response Answer:", vertexResponse.answer);
            console.log("[ChatAPI] Vertex Citations Count:", vertexResponse.citations.length);

            // Use the Vertex Answer as the primary source if available
            if (vertexResponse.answer) {
                vertexContext = `\n\n### ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æ¤œç´¢çµæœ (Vertex AI)\nVertex AI SearchãŒãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‹ã‚‰ä»¥ä¸‹ã®å›ç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚ã“ã‚Œã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”ã‚’è£œå®Œã—ã¦ãã ã•ã„ï¼š\n${vertexResponse.answer}`;

                // Append citations if they exist
                if (vertexResponse.citations.length > 0) {
                    const list = vertexResponse.citations.map(c => `[å‡ºå…¸:${c.title}]\n${c.contentSnippet}`).join('\n\n');
                    vertexContext += `\n\n#### å‚ç…§ã‚¹ãƒ‹ãƒšãƒƒãƒˆ:\n${list}`;
                }
            } else if (vertexResponse.citations.length > 0) {
                // Fallback to snippets if no generative answer but citations exist
                const list = vertexResponse.citations.map(c => `[å‡ºå…¸:${c.title}]\n${c.contentSnippet}`).join('\n\n');
                vertexContext = `\n\n### ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æ¤œç´¢çµæœ (Vertex AI)\nä»¥ä¸‹ã®å…¬å¼ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’**æœ€å„ªå…ˆã®æ ¹æ‹ **ã¨ã—ã¦å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š\n${list}`;
            } else {
                vertexContext = `\n\n### ãƒãƒ‹ãƒ¥ã‚¢ãƒ«æ¤œç´¢çµæœ\nè©²å½“ã™ã‚‹ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä¸€èˆ¬çš„ãªçŸ¥è­˜ã¾ãŸã¯å‰å¾Œã®æ–‡è„ˆã‹ã‚‰å›ç­”ã—ã¦ãã ã•ã„ã€‚`;
            }

            // Mentor Mode Logic
            let mentorContext = "";
            if (mentorMode) {
                mentorContext = `\n\n### ãƒ¡ãƒ³ã‚¿ãƒ¼ãƒ¢ãƒ¼ãƒ‰ (æ–°äººè·å“¡ã‚µãƒãƒ¼ãƒˆæ©Ÿèƒ½) ON ğŸ”°
ç¾åœ¨ã€ç›¸æ‰‹ã¯ã€Œæ–°äººè·å“¡ã€ã§ã™ã€‚ä»¥ä¸‹ã®è¿½åŠ æŒ‡ç¤ºã«å¾“ã£ã¦ãã ã•ã„ï¼š
- **å°‚é–€ç”¨èªã®å™›ã¿ç •ã**: è¡Œæ”¿ç”¨èªã‚„å°‚é–€ç”¨èªã‚’ä½¿ã†éš›ã¯ã€å¿…ãšã‚«ãƒƒã‚³æ›¸ãã§è£œè¶³èª¬æ˜ï¼ˆä¾‹ï¼šã€Œèµ·æ¡ˆï¼ˆç¨Ÿè­°æ›¸ã‚’ä½œã‚‹ã“ã¨ï¼‰ã€ï¼‰ã‚’åŠ ãˆã¦ãã ã•ã„ã€‚
- **èƒŒæ™¯ã®è£œè¶³**: å˜ã«ç­”ãˆã‚’æ•™ãˆã‚‹ã ã‘ã§ãªãã€ã€Œãªãœãã†ã™ã‚‹ã®ã‹ã€ã¨ã„ã†èƒŒæ™¯ã‚„æ–‡è„ˆã‚’ä¸å¯§ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
- **ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—**: æ‰‹é †ãŒè¤‡é›‘ãªå ´åˆã¯ã€ç•ªå·ä»˜ããƒªã‚¹ãƒˆã§ä¸€ã¤ãšã¤åˆ†è§£ã—ã¦æ¡ˆå†…ã—ã¦ãã ã•ã„ã€‚
- **åŠ±ã¾ã—**: ä¸å®‰ã‚’å–ã‚Šé™¤ããŸã‚ã€é€šå¸¸ã‚ˆã‚Šã‚‚æ¸©ã‹ãã€è‚¯å®šçš„ãªè¨€è‘‰æ›ã‘ã‚’æ„è­˜ã—ã¦ãã ã•ã„ã€‚`;
            }

            // Combine System Instruction
            const systemInstruction = `${systemPromptContent}
            
            ${vertexContext}
            
            ${knowledgeContext}
            
            ${mentorContext}`;

            // 5. Generate with LLM Service
            const llmService = container.llmService;

            // Construct prompt efficiently using only the latest query + instruction context
            // Or passing full history if supported by the service efficiently
            // Here we concatenate the system instruction to the latest prompt for simplicity with the stateless generic interface
            const finalPrompt = `${systemInstruction}\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: ${userQuery}`;

            // Filter history to last 5 turns to prevent token overflow
            const textHistory = messages.slice(-5);

            responseText = await llmService.generateCompletion(finalPrompt, textHistory);
            success = true;

            const citiedKnowledgeDocs = topKnowledge.map(k => ({
                id: k.id,
                author: 'System', // Supabase currently doesn't join user name easily here without new view
                content: k.summary || k.title
            }));

            // Include Vertex Citations in response for UI to display (as 'File' type citations)
            const finalCitations = vertexResponse.citations.map(c => ({
                id: c.id,
                title: c.title,
                text: c.contentSnippet
            }));

            return new Response(JSON.stringify({
                reply: responseText,
                citiedKnowledge: citiedKnowledgeDocs,
                vertexCitations: finalCitations
            }), {
                status: 200,
                headers: { "Content-Type": "application/json" },
            });

        } catch (llmError: any) {
            success = false;
            errorMessage = llmError instanceof Error ? llmError.message : String(llmError);
            console.error("LLM/RAG Error:", llmError);

            // Error handling logic
            if (errorMessage.includes("503") || errorMessage.includes("429")) {
                return new Response(JSON.stringify({
                    reply: "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨ã€çŸ¥æµã®æ£®ï¼ˆAIã‚µãƒ¼ãƒãƒ¼ï¼‰ãŒå¤§å¤‰æ··ã¿åˆã£ã¦ã„ã¾ã™ã€‚å°‘ã—æ™‚é–“ã‚’ãŠã„ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚ğŸ¦‰ğŸ’¦"
                }), { status: 200, headers: { "Content-Type": "application/json" } });
            }

            return new Response(JSON.stringify({ error: "Failed to generate content" }), {
                status: 500,
                headers: { "Content-Type": "application/json" },
            });
        }

    } catch (error) {
        console.error("API Route Error:", error);
        return new Response(JSON.stringify({ error: "Internal Server Error" }), {
            status: 500,
            headers: { "Content-Type": "application/json" },
        });
    }
}
